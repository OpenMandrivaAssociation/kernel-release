diff -up linux-5.0-rc1/fs/file.c.0341~ linux-5.0-rc1/fs/file.c
--- linux-5.0-rc1/fs/file.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/fs/file.c	2019-01-07 05:36:42.099652386 +0100
@@ -409,6 +409,7 @@ struct files_struct *get_files_struct(st
 
 	return files;
 }
+EXPORT_SYMBOL_GPL(get_files_struct);
 
 void put_files_struct(struct files_struct *files)
 {
@@ -421,6 +422,7 @@ void put_files_struct(struct files_struc
 		kmem_cache_free(files_cachep, files);
 	}
 }
+EXPORT_SYMBOL_GPL(put_files_struct);
 
 void reset_files_struct(struct files_struct *files)
 {
@@ -533,6 +535,7 @@ out:
 	spin_unlock(&files->file_lock);
 	return error;
 }
+EXPORT_SYMBOL_GPL(__alloc_fd);
 
 static int alloc_fd(unsigned start, unsigned flags)
 {
@@ -606,6 +609,7 @@ void __fd_install(struct files_struct *f
 	rcu_assign_pointer(fdt->fd[fd], file);
 	rcu_read_unlock_sched();
 }
+EXPORT_SYMBOL_GPL(__fd_install);
 
 void fd_install(unsigned int fd, struct file *file)
 {
diff -up linux-5.0-rc1/kernel/fork.c.0341~ linux-5.0-rc1/kernel/fork.c
--- linux-5.0-rc1/kernel/fork.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/kernel/fork.c	2019-01-07 05:36:42.099652386 +0100
@@ -1084,6 +1084,7 @@ void mmput_async(struct mm_struct *mm)
 		schedule_work(&mm->async_put_work);
 	}
 }
+EXPORT_SYMBOL_GPL(mmput_async);
 #endif
 
 /**
diff -up linux-5.0-rc1/kernel/sched/core.c.0341~ linux-5.0-rc1/kernel/sched/core.c
--- linux-5.0-rc1/kernel/sched/core.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/kernel/sched/core.c	2019-01-07 05:36:42.101652417 +0100
@@ -3930,6 +3930,7 @@ int can_nice(const struct task_struct *p
 	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
 		capable(CAP_SYS_NICE));
 }
+EXPORT_SYMBOL_GPL(can_nice);
 
 #ifdef __ARCH_WANT_SYS_NICE
 
diff -up linux-5.0-rc1/kernel/signal.c.0341~ linux-5.0-rc1/kernel/signal.c
--- linux-5.0-rc1/kernel/signal.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/kernel/signal.c	2019-01-07 05:36:42.100652401 +0100
@@ -1312,6 +1312,7 @@ struct sighand_struct *__lock_task_sigha
 
 	return sighand;
 }
+EXPORT_SYMBOL_GPL(__lock_task_sighand);
 
 /*
  * send signal info to all the members of a group
diff -up linux-5.0-rc1/mm/memory.c.0341~ linux-5.0-rc1/mm/memory.c
--- linux-5.0-rc1/mm/memory.c.0341~	2019-01-07 05:36:42.100652401 +0100
+++ linux-5.0-rc1/mm/memory.c	2019-01-07 05:39:36.970341876 +0100
@@ -1364,6 +1364,7 @@ void zap_page_range(struct vm_area_struc
 	mmu_notifier_invalidate_range_end(&range);
 	tlb_finish_mmu(&tlb, start, range.end);
 }
+EXPORT_SYMBOL_GPL(zap_page_range);
 
 /**
  * zap_page_range_single - remove user pages in a given range
diff -up linux-5.0-rc1/mm/shmem.c.0341~ linux-5.0-rc1/mm/shmem.c
--- linux-5.0-rc1/mm/shmem.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/mm/shmem.c	2019-01-07 05:36:42.100652401 +0100
@@ -3988,6 +3988,7 @@ int shmem_zero_setup(struct vm_area_stru
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(shmem_zero_setup);
 
 /**
  * shmem_read_mapping_page_gfp - read into page cache, using specified page allocation flags.
diff -up linux-5.0-rc1/mm/vmalloc.c.0341~ linux-5.0-rc1/mm/vmalloc.c
--- linux-5.0-rc1/mm/vmalloc.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/mm/vmalloc.c	2019-01-07 05:36:42.101652417 +0100
@@ -1295,6 +1295,7 @@ int map_kernel_range_noflush(unsigned lo
 {
 	return vmap_page_range_noflush(addr, addr + size, prot, pages);
 }
+EXPORT_SYMBOL_GPL(map_kernel_range_noflush);
 
 /**
  * unmap_kernel_range_noflush - unmap kernel VM area
@@ -1435,6 +1436,7 @@ struct vm_struct *get_vm_area(unsigned l
 				  NUMA_NO_NODE, GFP_KERNEL,
 				  __builtin_return_address(0));
 }
+EXPORT_SYMBOL_GPL(get_vm_area);
 
 struct vm_struct *get_vm_area_caller(unsigned long size, unsigned long flags,
 				const void *caller)
diff -up linux-5.0-rc1/security/security.c.0341~ linux-5.0-rc1/security/security.c
--- linux-5.0-rc1/security/security.c.0341~	2019-01-07 02:08:20.000000000 +0100
+++ linux-5.0-rc1/security/security.c	2019-01-07 05:36:42.101652417 +0100
@@ -247,24 +247,28 @@ int security_binder_set_context_mgr(stru
 {
 	return call_int_hook(binder_set_context_mgr, 0, mgr);
 }
+EXPORT_SYMBOL_GPL(security_binder_set_context_mgr);
 
 int security_binder_transaction(struct task_struct *from,
 				struct task_struct *to)
 {
 	return call_int_hook(binder_transaction, 0, from, to);
 }
+EXPORT_SYMBOL_GPL(security_binder_transaction);
 
 int security_binder_transfer_binder(struct task_struct *from,
 				    struct task_struct *to)
 {
 	return call_int_hook(binder_transfer_binder, 0, from, to);
 }
+EXPORT_SYMBOL_GPL(security_binder_transfer_binder);
 
 int security_binder_transfer_file(struct task_struct *from,
 				  struct task_struct *to, struct file *file)
 {
 	return call_int_hook(binder_transfer_file, 0, from, to, file);
 }
+EXPORT_SYMBOL_GPL(security_binder_transfer_file);
 
 int security_ptrace_access_check(struct task_struct *child, unsigned int mode)
 {
diff -up linux-5.0-rc2/fs/file.c.omv~ linux-5.0-rc2/fs/file.c
--- linux-5.0-rc2/fs/file.c.omv~	2019-01-19 06:14:11.022236291 +0100
+++ linux-5.0-rc2/fs/file.c	2019-01-19 06:14:28.723464356 +0100
@@ -672,6 +672,7 @@ out_unlock:
 	*res = NULL;
 	return -ENOENT;
 }
+EXPORT_SYMBOL(__close_fd_get_file); /* for binder */
 
 void do_close_on_exec(struct files_struct *files)
 {
diff -up linux-5.0-rc2/kernel/task_work.c.omv~ linux-5.0-rc2/kernel/task_work.c
--- linux-5.0-rc2/kernel/task_work.c.omv~	2019-01-19 06:24:02.824990811 +0100
+++ linux-5.0-rc2/kernel/task_work.c	2019-01-19 06:19:04.133049259 +0100
@@ -40,6 +40,7 @@ task_work_add(struct task_struct *task,
 		set_notify_resume(task);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(task_work_add); /* for binder */
 
 /**
  * task_work_cancel - cancel a pending work added by task_work_add()
